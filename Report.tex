% VDE Template for EUSAR Papers
% Provided by Barbara Lang und Siegmar Lampe
% University of Bremen, January 2002
% English version by Jens Fischer
% German Aerospace Center (DLR), December 2005
% Additional modifications by Matthias Wei{\ss}
% FGAN, January 2009

%-----------------------------------------------------------------------------
% Type of publication
\documentclass[a4paper,10pt]{article}
%-----------------------------------------------------------------------------
% Other packets: Most packets may be downloaded from www.dante.de and
% "tcilatex.tex" can be found at (December 2005):
% http://www.mackichan.com/techtalk/v30/UsingFloat.htm
% Not all packets are necessarily needed:
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
%\usepackage{ngerman} % in german language if required
\usepackage[nooneline,bf]{caption} % Figure descriptions from left margin
\usepackage{times}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[dvips]{graphicx}
\usepackage{epsfig}
\input{tcilatex}
%-----------------------------------------------------------------------------
% Page Setup
\textheight24cm \textwidth17cm \columnsep6mm
\oddsidemargin-5mm                 % depending on print drivers!
\evensidemargin-5mm                % required margin size: 2cm
\headheight0cm \headsep0cm \topmargin0cm \parindent0cm
\pagestyle{empty}                  % delete footer and header
%----------------------------------------------------------------------------
% Environment definitions
\newenvironment*{mytitle}{\begin{LARGE}\bf}{\end{LARGE}\\[1.5ex]}%
\newenvironment*{myabstract}{\begin{Large}\bf}{\end{Large}\\[2.5ex]}%
%-----------------------------------------------------------------------------
% Using Pictures and tables:
% - Instead "table" write "tablehere" without parameters
% - Instead "figure" write "figurehere " without parameters
% - Please insert a blank line before and after \begin{figuerhere} ... \end{figurehere}
%
% CAUTION:   The first reference to a figure/table in the text should be formatted fat.
%
\makeatletter
\newenvironment{tablehere}{\def\@captype{table}}{}
\newenvironment{figurehere}{\def\@captype{figure}\vspace{2ex}}{\vspace{2ex}}
\makeatother



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

% Please use capital letters in the beginning of important words as for example
\begin{mytitle} A LaTeX Report Template. \end{mytitle}
%
% Please do not insert a line here
%
\\
\hspace{10ex}
Marini Matteo\\
Matr. 724626\\[10ex]

\begin{myabstract} Abstract \end{myabstract}
This report explains how a kernel with a new patch is tested and 
which data and graphics are generated. It is explained how to insert new kernel to test
and it shows the workflow followed to compare vanilla kernel with new kernels

\vspace{4ex}	% Please do not remove or reduce this space here.
\begin{multicols}{2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

To understand how to improve kernel performance, it was necessary to develop a toolchain
that, for each kernel to test, it performs automatically necessary measures.
This toolchain select kernel to test, execute tests, select next kernel and reboot system.
All theese activities are performed automatically. User can decide which test to do, 
editing a configuration file present in toolchain. Detailed documentation of each script
used is in relative bash script file.
This report is divided in TODO numero sezioni sections: the first section presents a
briefly description of benchmark and tools used to measures performance of kernel, 
the second section describes structure of the toolchain,
the third section shows an high level description of bash scripts that performs
all measures, the fourth section describes which data are produced, how to use them
and logic used to build bar graphs. In the last section are examined possible use case
of toolchain

TODO: mettere le storie sulla possibile espansione della toolchain verso altri benchmark
magari come use case

TODO: per ora mi viene in mente solo qeusto use case

TODO: da qualche parte devo mettere la naming convention

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Benchmark and tool used to measure kernel improvements}
% Please avoid separations in titles
% and separate text manually

Application used as benchmark for kernel patch has this structure

TODO figure of benchmark, 

How application works is well documented in TODO metti ref alla tesi di lucas
Each sample is produced periodically by sink mixer. Monitor is task that launches "start"  
signal for production of a sample, therefore benchmark can be launched only with
monitor. TODO rivedi questa frase che forse non Ã¨ vera 
Number of sample (NR\_SAM) to produce is decided by it. 
Monitor measures production time of each sample. At the end of application 
monitor has recorded NR\_SAM times and it can provides 
\textbf{ average and variance} of theese measures. 
Theese statistical values are principal criteria to characterize 
kernel patches improvements.

Other important measures are obtained by combined use of monitor and ftrace.
With sched\_switch tracer, it is possible retrive averge execution time (AET) of each
thread present in benchmark, how? A bash script launches benchmark with monitor. 
Before to launch first "start" signal for benchmark, monitor enables sched\_switch tracer. 
Output of tracer is a file that shows timestamps of context switches and 
wakeups of each task of the benchmark. According by theese informations, it is possible 
compute average scheduling latency and AET of each task.
With function graph tracer it is possible calculate AET of each
call of a certain kernel functions. Also in this case, a bash script launches
benchmark with monitor, that enables funcgraph tracer before to send "start" signal
for the first benchmark sample. Output of tracer is a file that shows execution time employed by each
kernel function call and other informations such as: context within which function is called,
cpu that executed function etc..
According by theese informations, it is possible compute AET of each 
function call. 

It is clear that for each measure, it is necessary to launch benchmark and monitor by 
a bash script used as wrapper, in this way, produced data are managed by wrapper in order
to produce graphics, png etc..
In the toolchain there is one main test. We call main test a bash script that 
performs different measures, in plain words: a scripts that call more than one
scripts that wraps benchmark and monitor. Main test present in toolchain 
is called "Performace test" (ptest for briefly)
and it is implemented in run.sh. In the third section is explained how it works. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Toolchain structure}

Toolchain has structure see in 

TODO insert here figure of toolchain structure

\begin{description}
	\item[bmarks] folder that contains benchmark binaries 
	\item[images] folder that contains kernel images to test
	\item[scripts] folder that contains bash scripts to produce graphics and execute benchmark
	\item[results] folder that contains data produced
	\item[log] folder that contains test log files
\end{description}

results folder has a dynamic strcuture. It follows this schema:

TODO insert here figure of results structure

Every times that a main test is executed, a new folder in results directory is created.
Name of new folder is composed by date\_of\_today-version\_localversion of kernel in use
In this folder it will be created theese directories:

\begin{description}
	\item[data:] contains text files produced by different scripts called by main test executed
	\item[graphs:] contains plot files generated according to the files present in data folder
	\item[png:] contains png files produced by different scripts called by main test executed
\end{description}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ptest functioning}

ptest is main test that measures AET of a sample, AET of each task, load/store 
miss rate of benchmark, AET of kernel functions.

As I said above, user can edit parameters of ptest modifing a configuration file (init.env).
In this file are contained all parameters that describe structure of toolchain, name of files
used to reboot system and select kernel to test and parameters used to configure main test
ptest. For a detailed description of each parameter see init.env
User can modify init.env manually or automatically using init_env.sh.
In init_env.sh there are four execution profile for ptest:
\begin{description}
	\item[default] standard execution of ptest
	\item[performance] only bash scripts used to measure performance of benchmark are enabled 
	\item[function] only bash scripts used to measure performance of kernel function are enabled
	\item[experimental] user can specify what he wants
\end{description}

User is strongly encouraged to use init_env.sh to modify configuration file, for two reasons:
this procedure speed up editing of init.env and it prevent user to make any mistake, because init_env.sh
edit parameters related to execution of ptest, in this way structure of toolchain or name of different
files used aren't touched. 
Most important parameters used to configure ptest, they are:

\begin{description}
	\item[DIM\_LIST:] it is the list of buffer dimensions to use in benchmark
	\item[NR\_TRY\_PERFORMANCE\_TEST:] it is the number of iteration executed by ptest
\end{description}

we call NR\_DIM number of elements present in DIM\_LIST.
ptest follows this steps:

\begin{description}
	\item[Step1:] User specifies in init\_env.sh which desired execution profile
and execute init\_env.sh to configure init.env 
For example: User select deafault profile that set NR\_TRY\_PERFORMANCE\_TEST=10 and DIM\_LIST="4 8 16 32"
Pay attention to theese data, because they are taken as reference for next steps
	\item[Step2:] launch run.sh. This script, enter in a loop and for 
NR\_TRY\_PERFORMANCE\_TEST iterations executes enabled scripts. Each script, whatever it measures, 
receive as input paramters: number of iteration in execution, name of bencmark to execute, buffer dimension to use in benchmark. 
Then it launches benchmark with buffer dimension received. In our case 
at the first tim with 4, then with 8 and so on, therefore each script is executed 
NR\_TRY\_PERFORMANCE\_TEST x NR\_DIM. 
	\item[Step3:] build bar graphs. A script read data in data folder,
builds bar graphics and put them in graphics folder. Details about graphics are
in the next section
	\item[Step4:] select another kernel to test, update .status_bench and .running_bench, reboot system and go to Step2 
until all kernel in image folder are tested.
	\item[Step5:] build global graphics. According data in different data folders
created, builds "global" graphics

\end{description}

.status_bench and .running_bench are used by a deamon that at boot of system, continue execution of ptest 
Only thing at which user has to pay attention is to launch run.sh, only if kernel in use at that moment
is a kernel to test, for example if kernel to test are k_1, k_2 and k_3, kernel in use 
must be on of theese.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Produced data and graphics}

TODO metti una descrizione pi\'u ampia nella introduction

The toolchain can generate two type of bar graphs: they are "per dimension" and "per task". First type has on x axis buffer dimensions used.
Latter type has names of thread used in benchmark. Bar graphs are generated using \textbf{bargraph.pl}, a perl script 
that receive a text file that contains informations about title of graph, labels of axes, information related visualization of graph with gnuplot or 
other tools and obiouvsly data used to generate bar graph (see TODO url del bragraph for more details). File used by bargraph.pl is generated by
a bash script 

After execution of benchmark, each bash script used in ptest elaborates data given by benchmarks and insert computed values in a text file that 
now we call for briefly STATS\_FILE. This file will contains data and informations used by bargraph.pl to generate bar graph. Details on 
STATS\_FILE will be given in section.
When all bash scripts used un ptest finish their job, bash script used to generate file used by bargraph.pl is called. This script  perform for each STATS\_FILE generated a SQL-like query in order to retrieve data used to build bar graph. This procedure is executed for each STATS\_FILE and 
for each kernel tested. In this way, for each bar graph there will be a list of buffer dimension or a list of task name on x axis and for each element 
on x axis, there will be a number of column equal to number of kernel tested.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Scripts structure and file formats}

ptest calls different bash scripts to perform its measures. Each script receives in input:

\begin{enumerate}
	\item binary of benchmark to execute
	\item number of main test iteration
	\item dimension to use in benchmark
\end{enumerate}

and it follows this structure:

TODO metti il codice di un prototipo dello script

In the section 1), script imports configuration file to set up environment variables
that it use.

In the section 2), there is managing of input parameters and help for user.
As I said above, script interface is fixed and is composed by the parameteres previously
listed.

In the section 3), there is declaration of files produced by script.
A script can produce all files it wants, but it must produce the file declared in
STATS\_FILE.

In the section 4), there are definitions of headers used to generate bar graphs.

An header is composed by theese tags:

<TITLE>"Title of a bar graph"
<XLAB>"Label for x axis of a bar graph"
<YLAB>"Label for y axis of a bar graph"
<PREFIX\_PLOT>"name of file that contain data for a 'local' bar graphic"
<AVG\_COL\_TAG>"index of column that contains average values"
<VAR\_COL\_TAG>"index of column that contains variance values"

Data contained in header will be used to generate file for bargraph.pl. At each header corresponds one bar graph, a STATS\_FILE can
contains more than one header. Each script decides how many headers put in STATS\_FILE, and then how bar graphs to generate.
It is clear what each tag represents, but it's not immediate to understand how to use <AVG\_COL\_TAG> and <VAR\_COL\_TAG>. Now it's enough to know that
they are related to SQL-like query seen in previous section.

In the section 5), script writes necessary headers in STATS\_FILE.

In the section 6), script executes benchmark and enables tools used to perform measures

In the section 7), script writes measures in STATS\_FILE.

Each measure is inserted in a row tagged with <QUERY\_TAG>. A row follows this format:

<QUERY\_TAG>Average = "value" Variance = "value" Min = "value" Max = "value" \textit{other_values}
....

TODO quando dico bash script e intendo i\_bench.sh forse \'e meglio chiamarli measure_script

I will explain what contains QUERY\_TAG soon, now pay attention at how a bash script make a measure.
A bash script executes benchmark and enables tools needed to measure what it has to measure.
Theese tools can be sched_switch tracer, perf, monitor etc... and each of them produce a different output. Within a bash script,
a suitable awk script is called to elaborate output of theese tools and produce a generic list of values, then bash script computes average and variance
of theese values and then it arranges theese statistics in format showed above. Each bash script called in ptest analyzes at least a list of data, 
we call it "principal list", but some scripts have to do with more than one list, this fact depends from type
of file analyzed (perf output file, sched_switch tracer file etc..). For example: trace_bench.sh analyzes trace file produced
by sched_switch tracer. From this file, it extracts two lists and a scalar value related to each task: list of exec. time, list of scheduling
latency and number of migration.
Average and variance computed according to "principal list" are put respectively after "Average =" and "Variance =", while other computed values are
organized in arbitrary format decided within the script (\textit{other_values} indicates that in a row it is possible to have more than "standard" data)

QUERY\_TAG is used to perform SQL-like query necessary to build file for bargraph.pl. To understand how this tag works, we examine procedure to build
a "per dimension" bar graph. Considering example seen in section 1 TODO ref all'esempio.
Each script is executed 40 times (NR\_TRY\_PERFORMANCE\_TEST x NR\_DIM), and for each iteration
it writes an entry in its own STATS\_FILE, therefore there are 10 entries for each
dimension in DIM\_LIST. For this type of graph QUERY\_TAG has this format:

<buffer\_dim> where buffer\_dim is the dimension received in input by the script in execution
(and consequently used in benchmark)

in this way, it is easy select entries of each dimension, there will be 10 entries for each 
dimension considered, pick value at column indicates by AVG\_COL\_TAG and by VAR\_COL\_TAG
and compute mean of theese 10 values. In this way we have two values: first is mean
of averages and the latter is the mean of variances. Repeat this procedure for each dimension and finally we
will have NR\_DIM average and NR\_DIM variance. Note that a column in an entry is delimited by
a white space, therefore value of Average is in column 3, value of variance is in column 6 and so on.
In plain words, procedure is very similar to a SQL-like query.
Now it is clear the use of AVG\_COL\_TAG and by VAR\_COL\_TAG, they represent the index of columns used
in the projection of the query.

Procedure to build "per task" graph is the same, but in this case QUERY\_TAG has this format: 
<buffer\_dim><task\_name> in this case we group values according task name and buffer dimension


As mentioned before, for some script, an entry in STATS\_FILE presents aditional informations 
and not only "standard" data. When in STATS\_FILE, AVG\_COL\_TAG and VAR\_COL\_TAG indicates different indexes, our average
and variance values are means of two different list of values, as showed above. But if indexes are equal, namely
they indicates the same column, mean value computated is the average of a list of values, variance
is the variance of a list of values and not the mean as in previous case.


TODO use case su come introdurre nuove misure
Structure of the measure_script is quite flexible, if a new measure is introduced in ptest, 
the only thing to pay attention is how to 
"..ci vuole una nota che dica che il numero di colonna va riportato in init.env..." lo metto sempre nello use case

TODO alla fine parla di come si puÃ² rendere piÃ¹ flessible la toolchain
dicendo che se uno usa un altro benchmark puÃ² farlo purchÃ¨ produca una lista
di valori di qualunque natura essi siano: tempi percentuali ec..


% We suggest the use of JabRef for editing your bibliography file (Report.bib)
\bibliographystyle{splncs}
\bibliography{Report}

\end{multicols}
\end{document}
