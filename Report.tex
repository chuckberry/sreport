% VDE Template for EUSAR Papers
% Provided by Barbara Lang und Siegmar Lampe
% University of Bremen, January 2002
% English version by Jens Fischer
% German Aerospace Center (DLR), December 2005
% Additional modifications by Matthias Wei{\ss}
% FGAN, January 2009

%-----------------------------------------------------------------------------
% Type of publication
\documentclass[a4paper,10pt]{article}
%-----------------------------------------------------------------------------
% Other packets: Most packets may be downloaded from www.dante.de and
% "tcilatex.tex" can be found at (December 2005):
% http://www.mackichan.com/techtalk/v30/UsingFloat.htm
% Not all packets are necessarily needed:
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
%\usepackage{ngerman} % in german language if required
\usepackage[nooneline,bf]{caption} % Figure descriptions from left margin
\usepackage{times}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[dvips]{graphicx}
\usepackage{epsfig}
\input{tcilatex}
%-----------------------------------------------------------------------------
% Page Setup
\textheight24cm \textwidth17cm \columnsep6mm
\oddsidemargin-5mm                 % depending on print drivers!
\evensidemargin-5mm                % required margin size: 2cm
\headheight0cm \headsep0cm \topmargin0cm \parindent0cm
\pagestyle{empty}                  % delete footer and header
%----------------------------------------------------------------------------
% Environment definitions
\newenvironment*{mytitle}{\begin{LARGE}\bf}{\end{LARGE}\\[1.5ex]}%
\newenvironment*{myabstract}{\begin{Large}\bf}{\end{Large}\\[2.5ex]}%
%-----------------------------------------------------------------------------
% Using Pictures and tables:
% - Instead "table" write "tablehere" without parameters
% - Instead "figure" write "figurehere " without parameters
% - Please insert a blank line before and after \begin{figuerhere} ... \end{figurehere}
%
% CAUTION:   The first reference to a figure/table in the text should be formatted fat.
%
\makeatletter
\newenvironment{tablehere}{\def\@captype{table}}{}
\newenvironment{figurehere}{\def\@captype{figure}\vspace{2ex}}{\vspace{2ex}}
\makeatother



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

% Please use capital letters in the beginning of important words as for example
\begin{mytitle} A LaTeX Report Template. \end{mytitle}
%
% Please do not insert a line here
%
\\
\hspace{10ex}
Marini Matteo\\
Matr. 724626\\[10ex]

\begin{myabstract} Abstract \end{myabstract}
This report explains how a kernel with a new patch is tested and 
which data and graphics are generated. It is explained how to insert new kernel to test
and it shows the workflow followed to compare vanilla kernel with new kernels

\vspace{4ex}	% Please do not remove or reduce this space here.
\begin{multicols}{2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

To understand how to improve kernel performance, it was necessary to develop a toolchain
that, for each kernel to test, it performs automatically necessary measures.
This toolchain select kernel to test, execute tests, select next kernel and reboot system.
All theese activities are performed automatically. User can decide which test to do, 
editing a configuration file present in toolchain. Detailed documentation of each script
used is in relative bash script file.
This report is divided in TODO numero sezioni sections: the first section presents a
briefly description of benchmark and tools used to measures performance of kernel, 
the second section describes structure of the toolchain,
the third section shows an high level description of bash scripts that performs
all measures, the fourth section describes which data are produced, how to use them
and logic used to build bar graphs. In the last section are examined possible use case
of toolchain

TODO: mettere le storie sulla possibile espansione della toolchain verso altri benchmark
magari come use case

TODO: per ora mi viene in mente solo qeusto use case

TODO: da qualche parte devo mettere la naming convention

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Benchmark and tool used to measure kernel improvements}
% Please avoid separations in titles
% and separate text manually

Application used as benchmark for kernel patch has this structure

TODO figure of benchmark, 

How application works is well documented in TODO metti ref alla tesi di lucas
Each sample is produced periodically by sink mixer. Monitor is task that launches "start"  
signal for production of a sample, therefore benchmark can be launched only with
monitor. TODO rivedi questa frase che forse non è vera 
Number of sample (NR\_SAM) to produce is decided by it. 
Monitor measures production time of each sample. At the end of application 
monitor has recorded NR\_SAM times and it can provides 
\textbf{ average and variance} of theese measures. 
Theese statistical values are principal criteria to characterize 
kernel patches improvements.

Other important measures are obtained by combined use of monitor and ftrace.
With sched\_switch tracer, it is possible retrive averge execution time (AET) of each
thread present in benchmark, how? A bash script launches benchmark with monitor. 
Before to launch first "start" signal for benchmark, monitor enables sched\_switch tracer. 
Output of tracer is a file that shows timestamps of context switches and 
wakeups of each task of the benchmark. According by theese informations, it is possible 
compute average scheduling latency and AET of each task.
With function graph tracer it is possible calculate AET of each
call of a certain kernel functions. Also in this case, a bash script launches
benchmark with monitor, that enables funcgraph tracer before to send "start" signal
for the first benchmark sample. Output of tracer is a file that shows execution time employed by each
kernel function call and other informations such as: context within which function is called,
cpu that executed function etc..
According by theese informations, it is possible compute AET of each 
function call. 

It is clear that for each measure, it is necessary to launch benchmark and monitor by 
a bash script used as wrapper, in this way, produced data are managed by wrapper in order
to produce graphics, png etc..
In the toolchain there is one main test. We call main test a bash script that 
performs different measures, in plain words: a scripts that call more than one
scripts that wraps benchmark and monitor. Main test present in toolchain 
is called "Performace test" (ptest for briefly)
and it is implemented in run.sh. In the third section is explained how it works. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Toolchain structure}

Toolchain has structure see in 

TODO insert here figure of toolchain structure

\begin{description}
	\item[bmarks] folder that contains benchmark binaries 
	\item[images] folder that contains kernel images to test
	\item[scripts] folder that contains bash scripts to produce graphics and execute benchmark
	\item[results] folder that contains data produced
	\item[log] folder that contains test log files
\end{description}

results folder has a dynamic strcuture. It follows this schema:

TODO insert here figure of results structure

Every times that a main test is executed, a new folder in results directory is created.
Name of new folder is composed by date\_of\_today-version\_localversion of kernel in use
In this folder it will be created theese directories:

\begin{description}
	\item[data:] contains text files produced by different scripts called by main test executed
	\item[graphs:] contains plot files generated according to the files present in data folder
	\item[png:] contains png files produced by different scripts called by main test executed
\end{description}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ptest functioning}

ptest is main test that measures AET of a sample, AET of each task, load/store 
miss rate of benchmark, AET of kernel functions.

As I said above, user can edit parameters of ptest modifing a configuration file (init.env).
In this file are contained all parameters that describe structure of toolchain, name of files
used to reboot system and select kernel to test and parameters used to configure main test
ptest. For a detailed description of each parameter see init.env
User can modify init.env manually or automatically using init_env.sh.
In init_env.sh there are four execution profile for ptest:
\begin{description}
	\item[default] standard execution of ptest
	\item[performance] only bash scripts used to measure performance of benchmark are enabled 
	\item[function] only bash scripts used to measure performance of kernel function are enabled
	\item[experimental] user can specify what he wants
\end{description}

User is strongly encouraged to use init_env.sh to modify configuration file, for two reasons:
this procedure speed up editing of init.env and it prevent user to make any mistake, because init_env.sh
edit parameters related to execution of ptest, in this way structure of toolchain or name of different
files used aren't touched. 
Most important parameters used to configure ptest, they are:

\begin{description}
	\item[DIM\_LIST:] it is the list of buffer dimensions to use in benchmark
	\item[NR\_TRY\_PERFORMANCE\_TEST:] it is the number of iteration executed by ptest
\end{description}

we call NR\_DIM number of elements present in DIM\_LIST.
ptest follows this steps:

\begin{description}
	\item[Step1:] User specifies in init\_env.sh which desired execution profile
and execute init\_env.sh to configure init.env 
For example: User select deafault profile that set NR\_TRY\_PERFORMANCE\_TEST=10 and DIM\_LIST="4 8 16 32"
Pay attention to theese data, because they are taken as reference for next steps
	\item[Step2:] launch run.sh. This script, enter in a loop and for 
NR\_TRY\_PERFORMANCE\_TEST iterations executes enabled scripts. Each script, whatever it measures, 
receive as input paramters: number of iteration in execution, name of bencmark to execute, buffer dimension to use in benchmark. 
Then it launches benchmark with buffer dimension received. In our case 
at the first tim with 4, then with 8 and so on, therefore each script is executed 
NR\_TRY\_PERFORMANCE\_TEST x NR\_DIM. 
	\item[Step3:] build bar graphs. A script read data in data folder,
builds bar graphics and put them in graphics folder. Details about graphics are
in the next section
	\item[Step4:] select another kernel to test, update .status_bench and .running_bench, reboot system and go to Step2 
until all kernel in image folder are tested.
	\item[Step5:] build global graphics. According data in different data folders
created, builds "global" graphics

\end{description}

.status_bench and .running_bench are used by a deamon that at boot of system, continue execution of ptest 
Only thing at which user has to pay attention is to launch run.sh, only if kernel in use at that moment
is a kernel to test, for example if kernel to test are k_1, k_2 and k_3, kernel in use 
must be on of theese.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Produced data and graphics}

TODO separa le info legate al formato dello STATS\_FILE per fare i grafici 
dal resto

TODO usa sta frase da qui that has like
suffix something like this "\_stats.txt", prefix of file change according bash 
script that creates it a qui nella naming conventions

All bash scripts called by ptest, write their measures in a file. Each bash script 
has its own file. We call it STATS\_FILE 
Each STATS\_FILE follows this format:

\begin{description}
	\item[<TITLE>] Specifies title of bar graph to generate
	\item[<XLAB>] Specifies x label of bar graph to generate
	\item[<YLAB>] Specifies y label of bar graph to generate
	\item[<PREFIX\_PLOT>] Specifies name of "local" bar graphic
	\item[<AVG\_COL\_TAG>] Specifies index of column that contains average values
	\item[<VAR\_COL\_TAG>] Specifies index of column that contains variance values
\end{description}

All theese tags represent an "header" that will be read from bash script that builds 
"local" graphics. We use adjective "local" with a bar graph that shows data related only to 
running kernel when ptest was executed. At each header corresponds one "local" 
graphic, a STATS\_FILE can contains more than one header. 

After "headers" for bar graphs, there are data that follow theese format:

<QUERY\_TAG>Average = "value" Variance = "value" Min = "value" Max = "value" <other values>
....

TODO usa questa frase per dire che grazie alla list of values uno script può trattare
qualunque tipo di valore time percentage ecc..
theese values can be execution time of task, load/store miss rate etc...

I will explain <QUERY\_TAG> soon, now pay attention to the rest of string.
Each script analyzes a list of values and compute their average and variance,
Each script arranges theese computated data in format showed above.
<other_values> indicates that a script can provide not only average, variance and 
other statistical values, but also other values as: number of function call
number of migration of a task etc... it depends which is source of data examined.


TODO dire qualcosa sugli indici 

TODO nn ha tanto senso dire che ci sono 2 tipi di local graphs perchè 
anche i global graphics hanno 2 tipi

There are two types of "local" graphic: "per dimension"  and "per task"
First type has buffer dimension used on x axis, latter has threads name of benchmark on x axis.

Now I explain how to build a "per dimension" bar graph
TODO mettere riferimento a questo esempio
Considering previous example. Each script is executed 40 times
(NR\_TRY\_PERFORMANCE\_TEST x NR\_DIM), and for each iteration
it writes an entry in its own STATS\_FILE, therefore there are 10 entries for each
dimension in DIM\_LIST. For this type of graph QUERY\_TAG has this format

<buffer\_dim> where buffer\_dim is the dimension received in input by the script in execution
(and consequently used in benchmark)

in this way, it is easy select entries of each dimesion, there will be 10 entries for each 
dimension considered, pick value at column indicates by AVG\_COL\_TAG and by VAR\_COL\_TAG
and compute mean of theese 10 values, in this way we have two values: first is mean
of averages and the latter is the mean of variances. Repeat this procedure for each dimension and finally we
will have NR\_DIM average and NR\_DIM variance. Note that a column in an entry is delimited by
a white space, therefore value of Average is in column 3, value of variance is in column 6 and so on.
In plain words, procedure is very similar to a SQL query.

With computed values, we can build our "local" bar graph. Procedure to build
"per task" graph is the same, but in scripts that produce data for this type of bar graph,
QUERY\_TAG has this format: 

<buffer\_dim><task\_name> in this case we group values according task name and buffer dimension

Special cases:
As mentioned before, for some script, an entry in STATS\_FILE presents aditional informations 
and not only standard data. 
When in STATS\_FILE, AVG\_COL\_TAG and VAR\_COL\_TAG indicates different indexes, our average
and variance values are mean of a list of values, as shoed above. But if indexes are equal, namely 
they indicates the same column, mean value computated is the average of a list of values, variance
is the variance of a list of values and not the mean as in previous case.



% We suggest the use of JabRef for editing your bibliography file (Report.bib)
\bibliographystyle{splncs}
\bibliography{Report}

\end{multicols}
\end{document}
