% VDE Template for EUSAR Papers
% Provided by Barbara Lang und Siegmar Lampe
% University of Bremen, January 2002
% English version by Jens Fischer
% German Aerospace Center (DLR), December 2005
% Additional modifications by Matthias Wei{\ss}
% FGAN, January 2009

%-----------------------------------------------------------------------------
% Type of publication
\documentclass[a4paper,10pt]{article}
%-----------------------------------------------------------------------------
% Other packets: Most packets may be downloaded from www.dante.de and
% "tcilatex.tex" can be found at (December 2005):
% http://www.mackichan.com/techtalk/v30/UsingFloat.htm
% Not all packets are necessarily needed:
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
%\usepackage{ngerman} % in german language if required
\usepackage[nooneline,bf]{caption} % Figure descriptions from left margin
\usepackage{times}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[dvips]{graphicx}
\usepackage{epsfig}
\input{tcilatex}
%-----------------------------------------------------------------------------
% Page Setup
\textheight24cm \textwidth17cm \columnsep6mm
\oddsidemargin-5mm                 % depending on print drivers!
\evensidemargin-5mm                % required margin size: 2cm
\headheight0cm \headsep0cm \topmargin0cm \parindent0cm
\pagestyle{empty}                  % delete footer and header
%----------------------------------------------------------------------------
% Environment definitions
\newenvironment*{mytitle}{\begin{LARGE}\bf}{\end{LARGE}\\[1.5ex]}%
\newenvironment*{myabstract}{\begin{Large}\bf}{\end{Large}\\[2.5ex]}%
%-----------------------------------------------------------------------------
% Using Pictures and tables:
% - Instead "table" write "tablehere" without parameters
% - Instead "figure" write "figurehere " without parameters
% - Please insert a blank line before and after \begin{figuerhere} ... \end{figurehere}
%
% CAUTION:   The first reference to a figure/table in the text should be formatted fat.
%
\makeatletter
\newenvironment{tablehere}{\def\@captype{table}}{}
\newenvironment{figurehere}{\def\@captype{figure}\vspace{2ex}}{\vspace{2ex}}
\makeatother



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

% Please use capital letters in the beginning of important words as for example
\begin{mytitle} A LaTeX Report Template. \end{mytitle}
%
% Please do not insert a line here
%
\\
Marini Matteo\\
Matr. 724626\\
\hspace{10ex}

\begin{myabstract} Abstract \end{myabstract}

Multicore architectures, which contain multiple processing cores on a single chip, have been adopted by most chip manufacturers.
Dual-core chips are commonplace, and numerous four- and eight-core options exist. In the coming years, per-chip core counts will continue to increase.
The major obstacle to use multicores for real-time applications is that we may not predict and provide any guarantee on real-time properties 
of embedded software on such platforms; the way of handling the on-chip shared resources such as L2 cache may have a significant impact on the timing
predictability.

Recent works show how with a proper use of shared resource such as L2, it is possible improve predictability of tasks in order to perform 
system-level schedulability analysis for multicore systems based on task-level timing analysis us-ing existing WCET analysis techniques.
For this reason, it is necessary introduce new scheduling algorithms that consider how scheduled tasks use shared cache, in briefly 
it is necessary to develop "cache-aware" scheduling algorithms.

TODO quello che dimostra il modello LP è che è possibile usare queste timing techniques se si usa una partizionamento della L2

In this report, we show some examples of cache-aware scheduling algorithms and we show basic assumptions they are based on 

TODO sta cosa del linear problem non mi piace va scritta meglio, perchè risolvere quel problema vuol dire ricavare gli alfa e i beta (cioè dei tempi)
e quindi ci si riporta alla storia della timing analysis
and a linear model that describe the cache-aware scheduling problem, so when a task can be scheduled by this type of scheduling.

\vspace{4ex}	% Please do not remove or reduce this space here.
\begin{multicols}{2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Multicore architectures, which contain multiple processing cores on a single chip, have been adopted by most chip manufacturers.
Dual-core chips are commonplace, and numerous four and eight core options exist. In the coming years, per-chip core counts will continue to
increase for example, Intel has claimed that it will release 80-core chips as early as 2013. The shift to multicore technologies is a 
watershed event, as it fundamentally changes the "standard" computing platform in many settings to be a multiprocessor. In most multicore 
platforms, different cores share onchip caches. Without effective management by the scheduler, such caches can cause thrashing that 
severely degrades system performance. In fact, the issue of efficient cache
usage on multicore platforms is one of the most important problems with which chip makers are currently grappling.
It is predicted that multicores will be increasingly used in future embedded systems for high performance and low energy consumption. The
major obstacle is that we may not predict and provide any guarantee on real-time properties of embedded software on such platforms due to the on-chip
shared resources. Shared caches such as L2 or L3 cache are among the most critical resources on multicores, which severely degrade the timing
predictability of multicore systems due to the cache contention between cores. For single processor systems, 
\textbf{there are well-developed techniques for timing analysis of embedded software}. Using these techniques, the worst-case execution time 
(WCET) of real-time tasks may be estimated, and then used for system-level timing analyses like schedulability analysis. One major problem in 
WCET analysis is how to predict the cache behavior, since different cache behaviors (cache hit or miss) will result in different 
execution times of each instruction.

Unfortunately the existing techniques for single processor platforms are not applicable for multicores with shared caches. The reason is
that a task running on one core may evict the useful L2 cache content belonging to a task running on another core and therefore the
Worst-Case Execution Time (WCET) of one task can not be estimated in isolation from the other tasks as for single processor systems. 
Essentially, the challenge is to model and predict the cache behavior for concurrent programs (not sequential programs as for the case of 
single processor systems) running on different cores.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The first section: scheduling model}

In this section, we briefly describe the basic assumptions of the hardware platform and application tasks, considered to develop cache-aware 
scheduling algorithms

%-----------------------------------------------------------------------------
\subsection{The first subsection of the first}

We assume a multicore containing a fixed number of processor cores sharing an on-chip cache. Note that this is usually an L2 cache. We 
will not explicitly model core-local caches (usually L1) or other shared resources like interconnects. Since concurrent accesses to the shared 
cache give raise to the problem of reduced predictability due to cache interference, we assume the existence of a cache partitioning 
mechanism, allowing to divide the cache space into non-overlapping partitions for independent use by the computation tasks, TODO ref alla fig.
see Figure 1(a).
Assuming a k-associative cache that consists of l cache sets with k cache lines each, one can distinguish set-based and associativity-base
partitioning. The first one is also called row-based partitioning and assigns different cache sets to different partitions. It therefore enables
up to l partitions and is thus quite fine-grained for bigger caches. The second one assigns a certain amount of lines within each cache set 
to different partitions and is also called column-based partitioning, so it is rather coarse-grained with a maximum of just k partitions. 
Mixtures of both variants are also possible. TODO metti una roba del tipo "tra i p\'u famosi cache partitiong approcci ricordiamo il cache coloring"

%-----------------------------------------------------------------------------
\subsection{The second subsection of the first}

Assume a multicore platform consisting of $M$ cores and $A$ cache partitions, and a set $\tau$ of independent sporadic tasks
whose numbers of cache partitions (cache space size needed) and WCETs are known for the platform. We use $\tau_i = \langle A_i,C_i,D_i,T_i \rangle$
to denote such a task where $A_i$ is the \textit{cache space size}, $C_i$ is the \textit{worst-case execution time} (WCET), $D_i \le Ti$ is the 
relative deadline for each release, and $T_i$ is the minimum inter-arrival separation time also referred to as the period of the task. We further assume
that all tasks are ordered by priorities, i.e., $\tau_i$ has higher priority than $\tau_j$ iff $i < j$. The \textit{utilization} of a task $\tau_i$ is
$U_i = C_i/T_i$ and its slack $S_i = D_i - C_i$ , which is the longest delay allowed before actually running without missing its
deadline. A sporadic task $\tau_i$ generates a potentially infinite sequence of \textit{jobs} with successive job-arrivals separated by at least $T_i$
time units. The $\alpha^{th}$ job of task $\tau_i$ is denoted by $J_{i}^\alpha$, so we can denote a job sequence of task $\tau_i$
with $(J_{i}^1; J_{i}^2;\dots{})$. We omit $\alpha$ and just use $J_i$ to denote a job of $\tau_i$ if there is no need to identify which job it is. Each
job $J_i$ adheres to the conditions $A_i$, $C_i$ and $D_i$ of its task $\tau_i$ and has additional properties concerning \textit{absolute time points} 
related to its execution, which we denote with lower case letters: The \textit{release time}, denoted by $r_i$, the deadline, denoted by $d_i$
and derived using $d_i = r_i + D_i$, and the latest start time, denoted by $l_i$ and derived using $l_i = r_i + S_i$.
Finally, without loosing generality, we assume that time is dense in our model.

%-----------------------------------------------------------------------------
\subsection{The third subsection of the first: Cache-aware scheduling}

TODO: le assunzioni sono fatte, adesso descrivo il pattern generale seguito dagli scheduler dei paper, che è quello del modello teorico
TODO: cos\'i a gratis posso concludere il capitolo con il modello LP

TODO: Cosa dire: 
According to assumptions made, now it is possible to present the general pattern followed by all cache-aware scheduling algorithms analyzed.
All algorithms analyzed are nothing special. They have all characteristics of classic scheduling algorithms, such as priority inheritance, 
fixed priority etc.. and in addition they take care about what is memory region used by scheduled tasks
Analyzed algorithms are non-preemptive scheduling.

They follow theese rules: a job $J_i$ is scheduled for execution if:

\begin{enumerate}
	\item $J_i$ is the job of highest priority among all waiting jobs,
	\item There is at least one core idle
	\item There are enough cache partitions, i.e. at least $A_i$ , are available.
\end{enumerate}
  
Note that, since we suppose $D_i \le T_i$ for each task, there is at most one job of each task at any time instant.

TODO metti la fig 2 del LP

Figure 2 shows an example of the task set in Table 1 scheduled by this type of algorithms (the scenario that all tasks are released
together). Note that at time 0, the job $J_{4}^1$ can not execute according to the definition of the algorithm, although it is ready and
there is an idle core and enough idle cache partitions to fit it, since it is not at the first position of the waiting queue,
i.e. there is a higher priority job ($J_{1}^3$) waiting for execution. $J_{1}^3$ can not execute since there is not enough idle cache
partitions available. Thus, we note that FPCA may waste resources as it does not schedule lower priority ready jobs to execute in advance of 
higher priority ready jobs even though there are enough resources available to accomodate them.


TODO dico una roba del tipo: gli algor visti si differenziano per il profiling o per il coschedule




%-----------------------------------------------------------------------------
\subsection{The fifth subsection of the first: Lp formulation}

TODO questa subsec va alla fine dove dir\'o un vantaggio di qeusti algor. \'e che la predicib. aumenta e si pu\'o usare il modello LP 
TODO del paper ec..

The LP formulation will use the following constants:
\begin{itemize}
	\item $M$: the number of cores.
	\item $A$: the total number of partitions on the shared cache.
	\item $A_{i}$: the number of cache partitions occupied by each task $\tau_{i}$. (We also use the constant $A_{k}^{max}$, which is 
		     derived from these as above.)
	\item $I_{k}^i$: an upper bound of the interference by i in the problem window, which is computed as for each i.
\end{itemize}

Further, the following non-negative variables are used:

\begin{itemize}
 	\item $\alpha_i$: for each task $\tau_i$, we dene $\alpha_i$ as $\tau_i$'s accumulated execution time during $\beta$-intervals.
\end{itemize}

During the intervals, all $M$ cores are occupied. Further, we know that sum i i equals to the total computation work of $\sum_{i} \alpha_i$
all tasks during the intervals (which is the area with grids in Figure 5). We can therefore express the accumulated
length of all $\alpha$-intervals as:

\begin{center} $\frac{1}{M} \sum_{i} \alpha_i$ \end{center}

During the $\beta$-intervals, at least $A - A_{k}^{max} + 1$ cache partitions are occupied. Further, $\sum_{i} A_{i}\beta_{i}$ is the total is 
the total cache partition use of all tasks during the $\beta$-intervals (which is an upper bound of the area with diagonals in Figure 5). We
can therefore express an upper bound of the accumulated length of all $\beta$-intervals as:

\begin{center} $\frac{1}{A - A_{k}^{max} + 1}\sum_{i}A_{i}\beta_{i}$ \end{center}

Since $J_{k}$ is not schedulable, we further know that the sum of the accumulated lengths of the $\alpha-$ and $\beta$-intervals is at 
least $S_{k}$. Thus, using the expressions from (4) and (5), must hold that:

\begin{center}
\begin{equation*}
\sum_{i} \Bigl( \frac{1}{M}\alpha_{i} + \frac{A_{i}}{A - A_{k}^{max} + 1}\beta_{i}\Bigr) \ge S_{k}
\end{equation*}
\end{center}

We can use an LP solver to detect whether the i and variables can be chosen in a way to satisfy this condition.
If this is not the case, then k would be schedulable. We can use the object function of our LP formulation for that check:

\begin{center}
\begin{equation*}
Maximize \sum_{i} \Bigl( \frac{1}{M}\alpha_{i} + \frac{A_{i}}{A - A_{k}^{max} + 1}\beta_{i}\Bigr)
\end{equation*}
\end{center}

Thus, if the solution of the LP problem is smaller than $S_{k}$ we can determine that k is schedulable.
So far, the variables i and i are not bounded, so without further constraints, the LP formulation will not have a bounded solution (which 
would trivially render all tasks unschedulable). Therefore, we add constraints on the free variables, that follow directly from the structure 
of our schedulability problem. We have three constraints:

\begin{description}
\item[$\varphi_{1}$: Interference Constraint] We know that $I_{k}^j$ is the upper bound of the work done by $\tau_{j}$ in the problem 
window, so we have:
\begin{center} $\forall j: \alpha_{j} + \beta_{j} \le I_{k}^j$ \end{center}

\item[$\varphi_{2}$: Core Constraint] The work done by a task in the intervals can not be larger than the total accumulated length of the intervals
(see Expression (4)), so we have: 
\begin{center} $\forall j: \alpha_{j} \le \frac{1}{M} \sum_{i} \alpha_i$ \end{center}

\item[$\varphi_{2}$: Cache Constraint] 3: Cache Constraint The work done by a task in the intervals can not be larger than the total accumulated
length of the intervals. Thus, it can not be larger than the upper bound of the total length of the intervals (see Expression (5)), so we have:

\begin{center} $\forall j: \beta_{j} \le \frac{1}{A - A_{k}^{max} + 1} \sum_{i}A_{i}\beta_{i}$ \end{center}

\end{description}

To test $\tau_{k}$ for schedulability, we can now invoke an LP solver on the LP problem dened by constraints $\phi_{1}$ to $\phi_{3}$ and the 
object function in (7). By construction, we have a first schedulability test for $\tau$:

TODO magari lo tolgo sto teorema 
Theorem 1 (The First Test). For each task k, let
k denote the solution of the LP problem shown above. A 
task set is schedulable by FPCA, if for each task k 2 it 
holds that
k < Sk:

% We suggest the use of JabRef for editing your bibliography file (Report.bib)
\bibliographystyle{splncs}
\bibliography{Report}

\end{multicols}
\end{document}
